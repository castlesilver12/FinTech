{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c52c274-08e7-4d00-8a3e-e5a754870212",
   "metadata": {},
   "source": [
    "<font size=6><b>lec01.NLP 텍스트 전처리(Text preprocessing)\n",
    "\n",
    "<pre>\n",
    "01) 토큰화(Tokenization)\n",
    "\n",
    "02) 정제(Cleaning) and 정규화(Normalization)\n",
    "03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
    "04) 불용어(Stopword)\n",
    "05) 정규 표현식(Regular Expression)\n",
    "\n",
    "06) 정수 인코딩(Integer Encoding)\n",
    "08) 원-핫 인코딩(One-Hot Encoding)\n",
    "07) 패딩(Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad1e59-a67f-4536-941b-027f983bf520",
   "metadata": {},
   "source": [
    "* 딥 러닝을 이용한 자연어 처리 입문\n",
    "* https://wikidocs.net/book/2155"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0faa8-71e9-408f-b13d-f934f9dd4d10",
   "metadata": {},
   "source": [
    "* 자연어 처리(Natural Language Processing:NLP)\n",
    "    * 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a227f2f2-795c-4710-9688-14243a8fd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install jpype    #---비추 (whl 다운받아 설치, JAVA_HOME\\bin 환경변수)\n",
    "# # -------------------- kkoma tweepy   (4)\n",
    "# ! pip install konlpy   #---비추 (whl 다운받아 설치, tweepy==3.1x)\n",
    "# # -------------------- 별도 한글 사전, 형태소 분석기 설치\n",
    "# ! pip install mecab    #---비추 (whl 다운받아 설치, c:\\mecab 압축풀기:x86,dic)\n",
    "\n",
    "# # -------------------- 기타(띄어쓰기, PyKoSpacing:다운그래이드문제로 설치 XXX)\n",
    "# ! pip install kss\n",
    "# # -------------------- 기타(신조어)\n",
    "# ! pip install soynlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50577eca-5c3b-4476-868e-570c2b6f9664",
   "metadata": {},
   "source": [
    "# 토큰화(Tokenization)\n",
    "* 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업\n",
    "* 단어 토큰화 : 일반적으로 whitespace(띄어쓰기)로 나눔\n",
    "* 문장 토큰화 : 일반적으로 구두점(.)로 나눔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0333aa-afba-4e36-9770-2d5edde4d176",
   "metadata": {},
   "source": [
    "## nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfd5ee-4764-4725-8919-3e55cd1cf1f0",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c7278e3-7cc1-4c0a-807c-5790c6294037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c8d4e-10f7-497c-9645-fe0bc57f5175",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     C:\\Users\\677\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Unzipping tokenizers\\punkt.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db57d76c-a133-44e2-9aba-b462864566b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "sent = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "\n",
    "print('원본 :',sent)\n",
    "print('단어 토큰화1 :',word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d8f0-20bc-4990-bd22-c6f15f630d10",
   "metadata": {},
   "source": [
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b54d414-7e1c-4888-9593-1111d3f012a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화 list : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent = \"\"\"His barber kept his word. \n",
    "          But keeping such a huge secret to himself was driving him crazy. \n",
    "          Finally, the barber went up a mountain and almost to the edge of a cliff. \n",
    "          He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\"\"\n",
    "\n",
    "print('문장 토큰화 list :',sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76ce77d5-be91-4dbf-b1d2-ac2b376edff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 문장 토큰화 list : ['IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀.', '보내줘.', '그 후 점심 먹으러 가자.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀. 보내줘. 그 후 점심 먹으러 가자.\"\n",
    "print(' 문장 토큰화 list :',sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fba3953-9f70-4d1e-95ba-ffcc719becb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄바꿈하면 문장 분리가 안된다.\n",
    "# import kss\n",
    "\n",
    "# sent = \"\"\"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다! 이제 해보면 알걸요?\"\"\"\n",
    "# print('한국어 문장 토큰화 :',kss.split_sentences(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0cb3b-32ce-4c26-9ab6-379eafa2cb27",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417ffe1c-a480-4a77-b91b-e35d93cfa2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df5703-9a9c-46d8-8e01-81c2797c34d4",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "[nltk_data]     C:\\Users\\677\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be0ea27-514f-445d-bf9a-587b3c7f97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "sent = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "word_list = word_tokenize(sent)\n",
    "\n",
    "print('단어 토큰화 :', word_list)\n",
    "print('품사 태깅 :'  , pos_tag(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6f4d1-0878-4a41-b149-2c32348b2d0a",
   "metadata": {},
   "source": [
    "## KoNLPy(코엔엘파이)\n",
    "\n",
    "* 한국어 자연어 처리를 위한 파이썬 패키지\n",
    "* 형태소 분석기 :  Okt(Open Korea Text), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)\n",
    "* 메캅(Mecab) 별도 설치\n",
    "    * 1) morphs : 형태소 추출\n",
    "    * 2) pos : 품사 태깅(Part-of-speech tagging)\n",
    "    * 3) nouns : 명사 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11b66cf4-a202-4193-8d27-11d92fb3a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komoran 형태소 분석 : ['열심히', '코', '딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '아', '보', '아요']\n",
      "Komoran 품사 태깅 : [('열심히', 'MAG'), ('코', 'NNG'), ('딩', 'MAG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('당신', 'NNP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('아', 'EC'), ('보', 'VX'), ('아요', 'EC')]\n",
      "Komoran 명사 추출 : ['코', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "# from konlpy.tag import Okt\n",
    "# from konlpy.tag import Kkma\n",
    "\n",
    "komoran = Komoran()\n",
    "# okt = Okt()\n",
    "# kkma = Kkma()\n",
    "\n",
    "print('Komoran 형태소 분석 :', komoran.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "\n",
    "print('Komoran 품사 태깅 :'  , komoran.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('Komoran 명사 추출 :'  , komoran.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61fbc242-6698-4157-a414-c597c69c408f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : 열심히 코딩한 당신, 연휴에는 여행을 가봐요\n",
      "단어 토큰화1 : ['열심히', '코딩한', '당신', ',', '연휴에는', '여행을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sent = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "print('원본 :',sent)\n",
    "print('단어 토큰화1 :',word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf388a4-5477-41b0-af4a-da9637612e4c",
   "metadata": {},
   "source": [
    "## Mecab : 한글 강추\n",
    "* 사용법은 KoNLPy(코엔엘파이)와 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2cd0139-189d-4e9d-9191-d74b7e2dfe01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mecab 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에', '는', '여행', '을', '가', '봐요']\n",
      "Mecab 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('한', 'XSA+ETM'), ('당신', 'NP'), (',', 'SC'), ('연휴', 'NNG'), ('에', 'JKB'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가', 'VV'), ('봐요', 'EC+VX+EC')]\n",
      "Mecab 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# Mecab 사전\n",
    "# --------------------------------\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "\n",
    "print('Mecab 형태소 분석 :', mecab.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "\n",
    "print('Mecab 품사 태깅 :'  , mecab.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('Mecab 명사 추출 :'  , mecab.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a832b-8955-4898-852a-9548baf6f867",
   "metadata": {},
   "source": [
    "* 띄어쓰기에 민감하지 않다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efec7137-03d3-44ec-b2ba-6909be204cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가', '방', '에', '들어가', '신다']\n",
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EC')]\n"
     ]
    }
   ],
   "source": [
    "print( mecab.morphs(\"아버지가방에들어가신다\") )\n",
    "print(  mecab.pos(\"아버지가방에들어가신다\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb109e-b10a-4ced-9594-c5588f425e22",
   "metadata": {},
   "source": [
    "## keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ebadcdd-0a9f-486b-9e5f-e3ddb16b1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb79ff0a-8c6d-4417-983d-b2311e24a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras 토큰화 : ['열심히', '코딩한', '당신', '연휴에는', '여행을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "print('Keras 토큰화 :', text_to_word_sequence(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29ba389c-c320-4c35-a8bc-ec902d3b9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7a083920-77dd-4e65-ba0c-da6a8491343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"홍길동은 딥러닝을 공부한다.\"\n",
    "        \"홍길동은 딥러닝이 어렵다.\"\n",
    "        \"아무개도 딥러닝을 시작합니다.\"]\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8cd388c3-e137-4960-aa00-812b3d65c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서카운트 : 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"문서카운트 : {token.document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79013c3c-7e6d-46aa-955f-9a7054b8664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서카운트 : OrderedDict([('홍길동은', 2), ('딥러닝을', 2), ('공부한다', 1), ('딥러닝이', 1), ('어렵다', 1), ('아무개도', 1), ('시작합니다', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(f\"워드카운트(빈도) : {token.word_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "81b57017-6119-4f24-bfda-6cdeb1dd46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "워드에 인덱스 적용 : [[1, 2, 3, 1, 4, 5, 6, 2, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"워드에 인덱스 적용 : {token.texts_to_sequences(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4151f0c8-d452-414d-a011-cecb9d5bdc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서내 워드 출현 빈도 : defaultdict(<class 'int'>, {'아무개도': 1, '시작합니다': 1, '어렵다': 1, '공부한다': 1, '딥러닝을': 1, '홍길동은': 1, '딥러닝이': 1})\n"
     ]
    }
   ],
   "source": [
    "print(f\"문서내 워드 출현 빈도 : {token.word_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02b0d18e-8f98-4406-8e40-a44f233cb6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "홍길동은 딥러닝을 공부한다.홍길동은 딥러닝이 어렵다.아무개도 딥러닝을 시작합니다. [1, 2, 3, 1, 4, 5, 6, 2, 7]\n"
     ]
    }
   ],
   "source": [
    "text_seq_list = token.texts_to_sequences(text)\n",
    "for seq, sent in zip (text_seq_list,text) :\n",
    "    print(sent, seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a708d14-bb59-463e-9cdf-2db6a105639c",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf8326-dfd8-4797-8a8d-e628345fc2da",
   "metadata": {},
   "source": [
    "## 정제(Cleaning), 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c3ca6-435b-4937-a1d1-ff5a9ed23613",
   "metadata": {},
   "source": [
    "* 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
    "    * 불용어\n",
    "* 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.\n",
    "    * 어간(Stemming),표제어(Lemmatization) 추출\n",
    "    * 정규성 : 단어수 == 피쳐수 (복잡성을 줄이는 일)\n",
    "    * BoW(Bag of Words) : 단어의 빈도수를 기반 문서 내의 단어 수를 줄인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c75c3b-944c-492e-a73a-882de96e64fa",
   "metadata": {},
   "source": [
    "## 어간(Stemming),표제어(Lemmatization) 추출\n",
    "\n",
    "<pre>\n",
    "* 표제어(Lemma) : '표제어' 또는 '기본 사전형 단어' \n",
    "* 어간(Stem)    : 어간(stem)과 접사(affix) 중 어간 추출\n",
    "                : 단어의 의미를 담고 있는 단어의 핵심 부분\n",
    "                \n",
    "Lemmatization\n",
    "    am → be\n",
    "    the going → the going\n",
    "    having → have\n",
    "\n",
    "Stemming\n",
    "    am → am\n",
    "    the going → the go\n",
    "    having → hav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645124b-54c5-41e4-9e0f-77c9380758fe",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f5673-0cff-464e-94c9-377975d833c0",
   "metadata": {},
   "source": [
    "#### 표제어(Lemmatization) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1097b209-b4cb-4d51-bcbb-c3d23feb0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4c9a7-ed26-4a6d-a6ee-d3f91c808541",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[nltk_data] Downloading package wordnet to\n",
    "[nltk_data]     C:\\Users\\677\\AppData\\Roaming\\nltk_data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f7a7ab4-d711-41b1-8b6a-495b8c0f3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56514217-1513-4a17-bbef-3da6a9a97f0d",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[nltk_data] Downloading package omw-1.4 to\n",
    "[nltk_data]     C:\\Users\\677\\AppData\\Roaming\\nltk_data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0bacadc-fc62-4815-8104-706aa3904d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "#------------ 'lives' -->  'life'  -------\n",
    "print('표제어 추출 전 :',words)\n",
    "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bba4cfc-8d91-4151-85a4-fa19a19fa0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f337f-b6ee-4c9c-a282-81fcbb4483d4",
   "metadata": {},
   "source": [
    "#### 어간(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da7c413d-85a6-4f6e-a6cc-f0199a64c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'formalize', 'allowance', 'electricical', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'formal', 'allow', 'electric', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sent = \"This was formalize allowance electricical.\"\n",
    "word_list = word_tokenize(sent)\n",
    "\n",
    "print('어간 추출 전 :', word_list)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cb73f77-b5e2-419d-a27d-956be91acdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['나는', '빨간', '사과가', '좋아지려고', '합니다', '.']\n",
      "어간 추출 후 : ['나는', '빨간', '사과가', '좋아지려고', '합니다', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sent = \"나는 빨간 사과가 좋아지려고 합니다.\"\n",
    "word_list = word_tokenize(sent)\n",
    "\n",
    "print('어간 추출 전 :', word_list)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0804ef4-3952-4da1-96fa-c502b8605eb8",
   "metadata": {},
   "source": [
    "### Konlpy\n",
    "<pre>\n",
    "* 용언(동사, 형용사)에 주로 활용\n",
    "* 잡/어간 + 다/어미\n",
    "\n",
    "* 어간(stem) \n",
    "    원칙적으로 모양이 변하지 않는 부분\n",
    "    때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\n",
    "* 어미(ending)\n",
    "    어간 뒤에 붙어서 활용하면서 변하는 부분\n",
    "    여러 문법적 기능을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1cf52c23-fc8c-4e0e-a506-45d43e08f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '빨갛다', '사과', '가', '좋아지다', '하다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "sent = \"나는 빨간 사과가 좋아지려고 합니다.\"\n",
    "print( okt.morphs(sent, norm=True, stem=True) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50d9a5-a8a5-4d08-9c59-b927b085a617",
   "metadata": {},
   "source": [
    "## 불용어(Stopword)\n",
    "* 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들  \n",
    "    ref : https://github.com/stopwords-iso/stopwords-ko  \n",
    "    ./ko_stopwords.txt  \n",
    "* 1차 : 불용어사전(csv, txt)을 이용\n",
    "* 2차 : 최빈도(적다,많다) 제거\n",
    "* 3차 : 정규화(import re)를 통해 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f12c5-aabd-4a72-a8cc-03e6533a2a45",
   "metadata": {},
   "source": [
    "### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "438fc764-1b5e-4f8a-ac53-f60bdf7accd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbfa3f-f062-446b-a02d-c8bb4e6882f2",
   "metadata": {},
   "source": [
    "<pre>\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     C:\\Users\\677\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Unzipping corpora\\stopwords.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bda21f1-17e9-4d1b-8549-a27bcdd5cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words_list = stopwords.words('english')\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 10개 출력 :',stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10325355-2930-423d-a660-5aa551ee1cca",
   "metadata": {},
   "source": [
    "* 원리이해용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19aa8248-bef4-4282-b370-c18c54066d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_list.append(\"important\")\n",
    "\n",
    "# sent = \"Apple is not an important thing. It's everything.\"\n",
    "# word_list = word_tokenize(sent)\n",
    "\n",
    "# result = []\n",
    "# for word in word_list: \n",
    "#     if word not in stop_words_list: #씨잘데기없는게 아니면\n",
    "#         result.append(word) \n",
    "\n",
    "# print('불용어 제거 전 :', sent) \n",
    "# print('불용어 제거 후 :', result)\n",
    "\n",
    "# --------------------------------------\n",
    "# 불용어 제거 전 : Apple is not an important thing. It's everything.\n",
    "# 불용어 제거 후 : ['Apple', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a765f-c2b6-4a0f-871b-59c8911dfd27",
   "metadata": {},
   "source": [
    "### Mecap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8920fbc2-7b2d-4002-a469-73ad27b58ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : 고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든다.\n",
      "불용어 제거 후 : ['를', '아무렇', '게', '나', '구우', '려고', '하', '면', '.', '라고', '같', '은', '게', '아니', '거든', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든다.\"\n",
    "stop_word_list = [\"고기\", \"하면\",\"안\",\"돼\",\"다\"]\n",
    "\n",
    "# -----------------------------------------------\n",
    "# from konlpy.tag import Komoran\n",
    "# komoran = Komoran()\n",
    "# word_list = komoran.morphs(sent)  #형태소분석\n",
    "# -----------------------------------------------\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "word_list = mecab.morphs(sent)  #형태소분석\n",
    "\n",
    "result = [word for word in word_list if not word in stop_word_list]\n",
    "# result = []\n",
    "# for word in word_list: \n",
    "#     if word not in stop_words_list: #씨잘데기없는게 아니면\n",
    "#         result.append(word) \n",
    "\n",
    "print('불용어 제거 전 :' ,sent) \n",
    "print('불용어 제거 후 :' ,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f8731-9dee-4bf2-ad5a-93fa95855506",
   "metadata": {},
   "source": [
    "# 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49fa45-6a49-4185-964b-63805e3d1e25",
   "metadata": {},
   "source": [
    "## 단어인덱스(voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73e08204-4a97-49f7-88a9-02b1315ef47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0432cd81-5364-445f-b980-49d3b5eff716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'사과가': 2, '나는': 1, '빨간': 1, '좋아지려고': 1, '합니다': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "sent = \"나는 빨간 사과가 사과가 좋아지려고 합니다.\"\n",
    "word_list = word_tokenize(sent)\n",
    "vocab = Counter(word_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1d66fde-976d-460f-a9bd-e4c728dbc5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "워드에 인덱스 적용 : [[2, 3, 1, 1, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sent = [\"나는 빨간 사과가 사과가 좋아지려고 합니다.\"]  #---- 리스트타입\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(sent)     #---- fit_on_texts()\n",
    "print(f\"워드에 인덱스 적용 : {token.texts_to_sequences(sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5744c4ae-3d01-41ed-a533-992c9eb13c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'사과가': 1, '나는': 2, '빨간': 3, '좋아지려고': 4, '합니다': 5}\n"
     ]
    }
   ],
   "source": [
    "print(token.word_index)  #--- voca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22d02e-1dee-483f-8f44-43ed78869a86",
   "metadata": {},
   "source": [
    "## 원핫 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76cdd1-0ab1-43ab-9a2b-ab8ecdf807fd",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3775abe7-f3f1-4045-9d51-57a4aa4ae966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 1, 4, 5]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onehot</th>\n",
       "      <th>나는</th>\n",
       "      <th>빨간</th>\n",
       "      <th>사과가</th>\n",
       "      <th>좋아지려고</th>\n",
       "      <th>합니다</th>\n",
       "      <th>word_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>나는</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>빨간</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사과가</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사과가</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>좋아지려고</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>합니다</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  onehot   나는   빨간  사과가  좋아지려고  합니다  word_idx\n",
       "0     나는  1.0  0.0  0.0    0.0  0.0         2\n",
       "1     빨간  0.0  1.0  0.0    0.0  0.0         3\n",
       "2    사과가  0.0  0.0  1.0    0.0  0.0         1\n",
       "3    사과가  0.0  0.0  1.0    0.0  0.0         1\n",
       "4  좋아지려고  0.0  0.0  0.0    1.0  0.0         4\n",
       "5    합니다  0.0  0.0  0.0    0.0  1.0         5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "oh = OneHotEncoder(sparse=False)\n",
    "sent= '나는 빨간 사과가 사과가 좋아지려고 합니다.'\n",
    "df = pd.DataFrame({'onehot':text_to_word_sequence(sent)})\n",
    "\n",
    "\n",
    "\n",
    "temp = df[\"onehot\"].values.reshape(-1,1)\n",
    "\n",
    "oh.fit(temp)  #----------------------------리턴이 없다 주의주의주의주의\n",
    "oh_res = oh.transform(temp)\n",
    "ohdf = pd.DataFrame(oh_res,  columns=oh.categories_[0])\n",
    "ohdf = pd.concat([df['onehot'],  ohdf], axis=1)\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([sent])   \n",
    "print(token.texts_to_sequences([sent])[0])\n",
    "ohdf[\"word_idx\"] = token.texts_to_sequences([sent])[0]\n",
    "\n",
    "ohdf.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8882e0c8-8d45-4416-a1d5-bf1b34e8cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'사과가': 1, '나는': 2, '빨간': 3, '좋아지려고': 4, '합니다': 5, '빨갛다': 6}\n",
      "[[2, 3, 1, 1, 4, 5], [1, 6]]\n",
      "최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "oh = OneHotEncoder(sparse=False)\n",
    "sent= ['나는 빨간 사과가 사과가 좋아지려고 합니다.','사과가 빨갛다']\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(sent)   \n",
    "voca = token.word_index\n",
    "print(voca)\n",
    "\n",
    "encoding = token.texts_to_sequences(sent)\n",
    "print(encoding)\n",
    "\n",
    "max_len = max(len(item) for item in encoding)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3caca03d-f667-4cc6-b2c4-5db23d2502b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 1, 1, 4, 5],\n",
       "       [1, 6, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(encoding, padding=\"post\")  #padding=\"pre\"\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "edf3da3b-cc32-4db4-bd9d-dc10f5fe61b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5\n",
       "0  2  3  1  1  4  5\n",
       "1  1  6  0  0  0  0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(padded)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b8218631-09ef-4398-bf24-cc8c0f4aaea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6)\n",
      "[[0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5\n",
       "0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "2  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  1.0  0.0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "oh_arr = to_categorical(df.iloc[0])\n",
    "print(oh_arr.shape)\n",
    "print(oh_arr)\n",
    "df = pd.DataFrame(oh_arr)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "72f26ee0-ad29-4b50-9366-0a90f22a67a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12)\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (1, 6)\t1.0\n",
      "  (1, 8)\t1.0\n",
      "  (1, 10)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1)\\t1.0\\n  (0, 2)\\t1.0\\n  (0, 5)\\t1.0\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 0)\\t1.0\\n  (0, 3)\\t1.0\\n  (0, 4)\\t1.0\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0    (0, 1)\\t1.0\\n  (0, 2)\\t1.0\\n  (0, 5)\\t1.0\\n ...\n",
       "1    (0, 0)\\t1.0\\n  (0, 3)\\t1.0\\n  (0, 4)\\t1.0\\n ..."
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = OneHotEncoder()\n",
    "oh_arr = oh.fit_transform(df)  #----2D\n",
    "print(oh_arr.shape)\n",
    "print(oh_arr)\n",
    "df = pd.DataFrame(oh_arr)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71389a-54b0-42b6-875c-96ec72225cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412156-af4a-43cf-bf64-92414b8dcdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
